{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5692975,"sourceType":"datasetVersion","datasetId":3273348}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nfrom tqdm import tqdm\n\nBASE_INPUT_DIR = None\noriginal_biome_names = []\n\ntry:\n    dataset_root_walk = next(os.walk('/kaggle/input')) \n    if not dataset_root_walk[1] and dataset_root_walk[0] == '/kaggle/input':\n         dataset_root_walk = next(os.walk(dataset_root_walk[0]))\n\n    dataset_root_dir = dataset_root_walk[0]\n    preprocessed_data_path = os.path.join(dataset_root_dir, 'preprocessed_data')\n    \n    if os.path.isdir(preprocessed_data_path):\n        BASE_INPUT_DIR = preprocessed_data_path\n        original_biome_names = next(os.walk(BASE_INPUT_DIR))[1]\n        print(f\"Directorio base de entrada detectado: {BASE_INPUT_DIR}\")\n        print(f\"Se encontraron {len(original_biome_names)} directorios 'biome_X'.\")\n    else:\n        print(\"Advertencia: No se encontró 'preprocessed_data'. Buscando biomas en el directorio raíz del dataset.\")\n        BASE_INPUT_DIR = dataset_root_dir\n        original_biome_names = dataset_root_walk[1]\n        if not any(name.startswith('biome_') for name in original_biome_names):\n            raise FileNotFoundError(\"No se encontró 'preprocessed_data' ni directorios 'biome_X' en la raíz.\")\n            \nexcept (StopIteration, FileNotFoundError) as e:\n    print(f\"Error: No se encontró una estructura de directorios válida. {e}\")\n    print(\"Asegúrate de que el dataset (con 'preprocessed_data') esté agregado a este notebook.\")\n\nBASE_OUTPUT_DIR = '/kaggle/working/dataset_agrupado'\nos.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\nprint(f\"El dataset agrupado se guardará en: {BASE_OUTPUT_DIR}\")\n\n\nBIOME_MAP_BY_ID = {\n    \"Taiga\": [\"biome_5\", \"biome_19\", \"biome_32\", \"biome_33\", \"biome_133\"],\n    \"Taiga Nevada\": [\"biome_30\", \"biome_31\", \"biome_158\"],\n    \"Savana\": [\"biome_35\", \"biome_36\"],\n    \"Jungla\": [\"biome_21\", \"biome_22\"],\n    \"Bosque de Roble Oscuro\": [\"biome_29\", \"biome_157\"],\n    \"Desierto\": [\"biome_2\", \"biome_17\", \"biome_130\"],\n    \"Badlands\": [\"biome_37\", \"biome_38\", \"biome_39\"],\n    \"Bosque de Abeto\": [\"biome_27\", \"biome_28\", \"biome_156\"],\n    \"Pantano\": [\"biome_6\"],\n    \"Bosque de Roble\": [\"biome_4\", \"biome_132\"],\n    \"Planicies\": [\"biome_1\", \"biome_129\"],\n    \"Bosque Mixto\": [\"biome_18\", \"biome_34\"],\n    \"Tundra Nevada\": [\"biome_12\"],\n    \"Montañas\": [\"biome_3\", \"biome_131\", \"biome_162\"],\n    \"Montaña Nevada\": [\"biome_13\"],\n    \"Playa\": [\"biome_16\", \"biome_26\"],\n    \"Ríos\": [\"biome_7\", \"biome_11\"]\n}\n\nBIOMAS_EXCLUIDOS_BY_ID = [\"biome_10\", \"biome_45\"]\n\ntotal_images_processed = 0\nnew_category_counts = {category: 0 for category in BIOME_MAP_BY_ID.keys()}\noriginal_biomes_mapped = set()\n\nfor new_category, original_biome_id_list in tqdm(BIOME_MAP_BY_ID.items(), desc=\"Agrupando categorías\"): \n    new_category_dir = os.path.join(BASE_OUTPUT_DIR, new_category)\n    os.makedirs(new_category_dir, exist_ok=True)\n    \n    for old_biome_dir_name in original_biome_id_list:\n        original_biome_path = os.path.join(BASE_INPUT_DIR, old_biome_dir_name)\n        original_biomes_mapped.add(old_biome_dir_name)\n        \n        if not os.path.isdir(original_biome_path):\n            print(f\"  -> Advertencia: No se encontró el directorio para '{old_biome_dir_name}'. Omitiendo.\")\n            continue\n            \n        try:\n            images = os.listdir(original_biome_path)\n            for image_name in images:\n                source_path = os.path.join(original_biome_path, image_name)\n                dest_name = f\"{old_biome_dir_name}_{image_name}\"\n                dest_path = os.path.join(new_category_dir, dest_name)\n                \n                try:\n                    os.symlink(source_path, dest_path)\n                    total_images_processed += 1\n                    new_category_counts[new_category] += 1\n                except FileExistsError:\n                    pass \n                except Exception as e:\n                    print(f\"Error al crear enlace para {source_path}: {e}\")\n\n        except Exception as e:\n            print(f\"  -> Error procesando '{original_biome_path}': {e}\")\n\nprint(\"\\n--- Proceso de agrupación completado ---\")\nprint(f\"Total de imágenes enlazadas: {total_images_processed}\")\n\ntarget_counts = {\n    \"Taiga\": 2346, \"Taiga Nevada\": 420, \"Savana\": 1164, \"Jungla\": 444,\n    \"Bosque de Roble Oscuro\": 870, \"Desierto\": 2088, \"Badlands\": 210,\n    \"Bosque de Abeto\": 1122, \"Pantano\": 665, \"Bosque de Roble\": 2850,\n    \"Planicies\": 3186, \"Bosque Mixto\": 1320, \"Tundra Nevada\": 1254,\n    \"Montañas\": 2652, \"Montaña Nevada\": 474, \"Playa\": 540, \"Ríos\": 444\n}\nprint(\"\\n--- Verificación de Conteo de Imágenes ---\")\nfor category, count in new_category_counts.items():\n    target = target_counts.get(category, 0)\n    discrepancy = \"\"\n    if count != target:\n        discrepancy = f\" (Esperado: {target} - ¡Revisar!)\"\n    print(f\"- {category}: {count} imágenes {discrepancy}\")\n\nall_original_biomes_in_map = original_biomes_mapped | set(BIOMAS_EXCLUIDOS_BY_ID)\nunmapped_biomes = [b for b in original_biome_names if b not in all_original_biomes_in_map]\n        \nif unmapped_biomes:\n    print(f\"\\n¡ADVERTENCIA! Los siguientes directorios no fueron mapeados ni excluidos:\")\n    for b in unmapped_biomes: print(f\"  - {b}\")\nelse:\n    print(\"\\nTodos los directorios de biomas originales fueron mapeados o excluidos correctamente.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T18:08:23.141560Z","iopub.execute_input":"2025-11-02T18:08:23.142472Z","iopub.status.idle":"2025-11-02T18:08:23.373368Z","shell.execute_reply.started":"2025-11-02T18:08:23.142439Z","shell.execute_reply":"2025-11-02T18:08:23.372367Z"}},"outputs":[{"name":"stdout","text":"Directorio base de entrada detectado: /kaggle/input/preprocessed_data\nSe encontraron 41 directorios 'biome_X'.\nEl dataset agrupado se guardará en: /kaggle/working/dataset_agrupado\n","output_type":"stream"},{"name":"stderr","text":"Agrupando categorías: 100%|██████████| 17/17 [00:00<00:00, 88.86it/s]","output_type":"stream"},{"name":"stdout","text":"\n--- Proceso de agrupación completado ---\nTotal de imágenes enlazadas: 0\n\n--- Verificación de Conteo de Imágenes ---\n- Taiga: 0 imágenes  (Esperado: 2346 - ¡Revisar!)\n- Taiga Nevada: 0 imágenes  (Esperado: 420 - ¡Revisar!)\n- Savana: 0 imágenes  (Esperado: 1164 - ¡Revisar!)\n- Jungla: 0 imágenes  (Esperado: 444 - ¡Revisar!)\n- Bosque de Roble Oscuro: 0 imágenes  (Esperado: 870 - ¡Revisar!)\n- Desierto: 0 imágenes  (Esperado: 2088 - ¡Revisar!)\n- Badlands: 0 imágenes  (Esperado: 210 - ¡Revisar!)\n- Bosque de Abeto: 0 imágenes  (Esperado: 1122 - ¡Revisar!)\n- Pantano: 0 imágenes  (Esperado: 665 - ¡Revisar!)\n- Bosque de Roble: 0 imágenes  (Esperado: 2850 - ¡Revisar!)\n- Planicies: 0 imágenes  (Esperado: 3186 - ¡Revisar!)\n- Bosque Mixto: 0 imágenes  (Esperado: 1320 - ¡Revisar!)\n- Tundra Nevada: 0 imágenes  (Esperado: 1254 - ¡Revisar!)\n- Montañas: 0 imágenes  (Esperado: 2652 - ¡Revisar!)\n- Montaña Nevada: 0 imágenes  (Esperado: 474 - ¡Revisar!)\n- Playa: 0 imágenes  (Esperado: 540 - ¡Revisar!)\n- Ríos: 0 imágenes  (Esperado: 444 - ¡Revisar!)\n\nTodos los directorios de biomas originales fueron mapeados o excluidos correctamente.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, optimizers\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn.metrics as metrics\nimport seaborn as sns\nimport os\nimport sys\n\nIMG_HEIGHT = 180\nIMG_WIDTH = 320\nBATCH_SIZE = 32\nVALIDATION_SPLIT = 0.2\nSEED = 123\n\nDATA_DIR = '/kaggle/working/dataset_agrupado'\n\nif not os.path.exists(DATA_DIR) or not os.listdir(DATA_DIR):\n    print(f\"Error: El directorio '{DATA_DIR}' está vacío o no existe.\")\n    print(\"Por favor, ejecuta la Celda 1 (agrupación de dataset) primero.\")\nelse:\n    CLASS_NAMES = sorted(os.listdir(DATA_DIR))\n    NUM_CLASSES = len(CLASS_NAMES)\n    print(f\"Configuración lista.\")\n    print(f\"Clases detectadas ({NUM_CLASSES}): {CLASS_NAMES}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T18:08:23.375079Z","iopub.execute_input":"2025-11-02T18:08:23.375390Z","iopub.status.idle":"2025-11-02T18:08:23.383058Z","shell.execute_reply.started":"2025-11-02T18:08:23.375363Z","shell.execute_reply":"2025-11-02T18:08:23.382267Z"}},"outputs":[{"name":"stdout","text":"Configuración lista.\nClases detectadas (17): ['Badlands', 'Bosque Mixto', 'Bosque de Abeto', 'Bosque de Roble', 'Bosque de Roble Oscuro', 'Desierto', 'Jungla', 'Montaña Nevada', 'Montañas', 'Pantano', 'Planicies', 'Playa', 'Ríos', 'Savana', 'Taiga', 'Taiga Nevada', 'Tundra Nevada']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"target_counts = {\n    \"Taiga\": 2346, \"Taiga Nevada\": 420, \"Savana\": 1164, \"Jungla\": 444,\n    \"Bosque de Roble Oscuro\": 870, \"Desierto\": 2088, \"Badlands\": 210,\n    \"Bosque de Abeto\": 1122, \"Pantano\": 665, \"Bosque de Roble\": 2850,\n    \"Planicies\": 3186, \"Bosque Mixto\": 1320, \"Tundra Nevada\": 1254,\n    \"Montañas\": 2652, \"Montaña Nevada\": 474, \"Playa\": 540, \"Ríos\": 444\n}\n\nMINORITY_THRESHOLD = 1000\nprint(f\"Umbral para aumento de datos: {MINORITY_THRESHOLD} imágenes.\")\n\nprint(\"Cargando dataset de entrenamiento...\")\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    DATA_DIR,\n    validation_split=VALIDATION_SPLIT,\n    subset=\"training\",\n    seed=SEED,\n    image_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE\n)\n\nprint(\"Cargando dataset de validación...\")\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    DATA_DIR,\n    validation_split=VALIDATION_SPLIT,\n    subset=\"validation\",\n    seed=SEED,\n    image_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE\n)\n\nclass_names_loaded = train_ds.class_names\nminority_class_indices = []\n\nprint(\"Identificando clases minoritarias para aumento:\")\nfor i, class_name in enumerate(class_names_loaded):\n    count = target_counts.get(class_name, 0)\n    if count < MINORITY_THRESHOLD and count > 0:\n        minority_class_indices.append(i)\n        print(f\"  -> SÍ: {class_name} (índice {i}) con {count} imágenes.\")\n    else:\n        print(f\"  -> NO: {class_name} (índice {i}) con {count} imágenes.\")\n\nMINORITY_INDICES_TENSOR = tf.constant(minority_class_indices, dtype=tf.int32)\n\ndata_augmentation = keras.Sequential(\n    [\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.1),\n        layers.RandomZoom(0.1),\n        layers.RandomContrast(0.1),\n    ],\n    name=\"data_augmentation\",\n)\n\n# --- INICIO DE LA SECCIÓN CORREGIDA ---\n\n@tf.function\ndef augment_if_minority(x, y):\n    # x es un lote de imágenes [32, H, W, C]\n    # y es un lote de etiquetas [32]\n    \n    # 1. Generar la versión aumentada del lote completo\n    x_augmented = data_augmentation(x, training=True)\n    \n    # 2. Crear la máscara de condición\n    \n    # Convertir etiquetas a int32\n    y_cast = tf.cast(y, tf.int32)\n    \n    # Reformatear 'y' para broadcasting: [32] -> [32, 1]\n    y_reshaped = tf.reshape(y_cast, [-1, 1])\n    \n    # Reformatear 'minority_indices' para broadcasting: [8] -> [1, 8]\n    minority_reshaped = tf.reshape(MINORITY_INDICES_TENSOR, [1, -1])\n    \n    # Comparar [32, 1] con [1, 8]. Resultado: Matriz [32, 8] de True/False\n    comparison_matrix = tf.equal(y_reshaped, minority_reshaped)\n    \n    # Comprobar si *alguna* comparación fue True para cada imagen\n    # Resultado: Vector [32] (ej. [True, False, True...])\n    is_minority_vec = tf.reduce_any(comparison_matrix, axis=1)\n    \n    # 3. Reformatear la máscara para que coincida con las imágenes\n    # [32] -> [32, 1, 1, 1]\n    # Esto le dice a tf.where que use la imagen completa\n    condition = tf.reshape(is_minority_vec, [-1, 1, 1, 1])\n    \n    # 4. Usar tf.where\n    # Si condition[i] es True, toma de x_augmented[i]\n    # Si condition[i] es False, toma de x[i] (la original)\n    x_final = tf.where(condition, x_augmented, x)\n    \n    return x_final, y\n\n# --- FIN DE LA SECCIÓN CORREGIDA ---\n\n\nprint(\"\\nAplicando aumento condicional SOLO a las clases minoritarias...\")\n\ntrain_ds = train_ds.map(\n    augment_if_minority,\n    num_parallel_calls=tf.data.AUTOTUNE\n)\n\ntrain_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n\nprint(\"\\nDatasets de entrenamiento (condicional) y validación listos.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T18:08:23.383907Z","iopub.execute_input":"2025-11-02T18:08:23.384808Z","iopub.status.idle":"2025-11-02T18:08:29.167508Z","shell.execute_reply.started":"2025-11-02T18:08:23.384778Z","shell.execute_reply":"2025-11-02T18:08:29.166588Z"}},"outputs":[{"name":"stdout","text":"Umbral para aumento de datos: 1000 imágenes.\nCargando dataset de entrenamiento...\nFound 22049 files belonging to 17 classes.\nUsing 17640 files for training.\nCargando dataset de validación...\nFound 22049 files belonging to 17 classes.\nUsing 4409 files for validation.\nIdentificando clases minoritarias para aumento:\n  -> SÍ: Badlands (índice 0) con 210 imágenes.\n  -> NO: Bosque Mixto (índice 1) con 1320 imágenes.\n  -> NO: Bosque de Abeto (índice 2) con 1122 imágenes.\n  -> NO: Bosque de Roble (índice 3) con 2850 imágenes.\n  -> SÍ: Bosque de Roble Oscuro (índice 4) con 870 imágenes.\n  -> NO: Desierto (índice 5) con 2088 imágenes.\n  -> SÍ: Jungla (índice 6) con 444 imágenes.\n  -> SÍ: Montaña Nevada (índice 7) con 474 imágenes.\n  -> NO: Montañas (índice 8) con 2652 imágenes.\n  -> SÍ: Pantano (índice 9) con 665 imágenes.\n  -> NO: Planicies (índice 10) con 3186 imágenes.\n  -> SÍ: Playa (índice 11) con 540 imágenes.\n  -> SÍ: Ríos (índice 12) con 444 imágenes.\n  -> NO: Savana (índice 13) con 1164 imágenes.\n  -> NO: Taiga (índice 14) con 2346 imágenes.\n  -> SÍ: Taiga Nevada (índice 15) con 420 imágenes.\n  -> NO: Tundra Nevada (índice 16) con 1254 imágenes.\n\nAplicando aumento condicional SOLO a las clases minoritarias...\n\nDatasets de entrenamiento (condicional) y validación listos.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def build_model(num_classes):\n    model = models.Sequential(name=\"Minecraft_Biome_Classifier\")\n    \n    model.add(layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n    model.add(layers.Rescaling(1./255))\n\n    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n    model.add(layers.MaxPooling2D((2, 2)))\n\n    model.add(layers.Flatten())\n\n    model.add(layers.Dense(512, activation='relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(256, activation='relu'))\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Dense(num_classes, activation='softmax'))\n    \n    return model\n\nprint(\"Función 'build_model' definida.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T18:08:29.168412Z","iopub.execute_input":"2025-11-02T18:08:29.168679Z","iopub.status.idle":"2025-11-02T18:08:29.178770Z","shell.execute_reply.started":"2025-11-02T18:08:29.168659Z","shell.execute_reply":"2025-11-02T18:08:29.177091Z"}},"outputs":[{"name":"stdout","text":"Función 'build_model' definida.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def train_model_interactive(model, train_ds, val_ds, existing_history=None):\n    try:\n        epochs = int(input(\"Ingrese el número de épocas (ej. 10): \") or 10)\n        lr = float(input(\"Ingrese la tasa de aprendizaje (ej. 0.001): \") or 0.001)\n    except ValueError:\n        print(\"Error: Entrada no válida.\")\n        return model, existing_history\n\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=lr),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    print(f\"\\nEntrenando por {epochs} épocas con LR={lr}...\")\n    \n    history = model.fit(\n        train_ds,\n        epochs=epochs,\n        validation_data=val_ds,\n        verbose=1\n    )\n    \n    print(\"Entrenamiento finalizado.\")\n    \n    if existing_history:\n        for key in history.history:\n            if key in existing_history.history:\n                existing_history.history[key].extend(history.history[key])\n            else:\n                existing_history.history[key] = history.history[key]\n        plot_loss_history(existing_history)\n        return model, existing_history\n    else:\n        plot_loss_history(history)\n        return model, history\n\ndef plot_loss_history(history):\n    acc = history.history.get('accuracy', [])\n    val_acc = history.history.get('val_accuracy', [])\n    loss = history.history.get('loss', [])\n    val_loss = history.history.get('val_loss', [])\n    \n    if not acc or not val_acc or not loss or not val_loss:\n        print(\"No hay suficiente historial para graficar.\")\n        return\n\n    epochs_range = range(len(acc))\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, acc, label='Precisión de Entrenamiento')\n    plt.plot(epochs_range, val_acc, label='Precisión de Validación')\n    plt.legend(loc='lower right')\n    plt.title('Precisión de Entrenamiento y Validación')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Pérdida de Entrenamiento')\n    plt.plot(epochs_range, val_loss, label='Pérdida de Validación')\n    plt.legend(loc='upper right')\n    plt.title('Pérdida de Entrenamiento y Validación')\n    plt.show()\n\ndef test_model_interactive(model, val_ds, class_names_list):\n    print(\"Evaluando modelo contra el conjunto de validación...\")\n    \n    loss, accuracy = model.evaluate(val_ds, verbose=0)\n    print(\"\\n\" + \"=\"*60)\n    print(\"          RESULTADOS DE LA EVALUACIÓN\")\n    print(\"=\"*60)\n    print(f\"Pérdida (Loss): {loss:.4f}\")\n    print(f\"Precisión (Accuracy): {accuracy * 100:.2f}%\")\n    \n    if accuracy > 0.90:\n        print(\"¡Éxito! Se alcanzó el objetivo de >90% de precisión.\")\n    else:\n        print(f\"Objetivo de >90% no alcanzado.\")\n    print(\"=\"*60)\n\n    print(\"\\nGenerando predicciones para el reporte detallado...\")\n    y_pred = []\n    y_true = []\n    for images, labels in val_ds:\n        y_true.extend(labels.numpy())\n        preds = model.predict(images, verbose=0)\n        y_pred.extend(np.argmax(preds, axis=1))\n        \n    print(\"\\n--- Reporte de Clasificación (Precisión, Recall, F1) ---\")\n    print(metrics.classification_report(y_true, y_pred, target_names=class_names_list, zero_division=0))\n\n    print(\"Generando Matriz de Confusión...\")\n    cm = metrics.confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(14, 12))\n    sns.heatmap(\n        cm, \n        annot=True, \n        fmt='d', \n        cmap='Blues', \n        xticklabels=class_names_list, \n        yticklabels=class_names_list\n    )\n    plt.title('Matriz de Confusión')\n    plt.ylabel('Bioma Real (True Label)')\n    plt.xlabel('Bioma Predicho (Predicted Label)')\n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0)\n    plt.show()\n\ndef save_model_interactive(model):\n    print(\"El modelo se guardará en '/kaggle/working/'\")\n    filename = input(\"Nombre del archivo (ej: mi_modelo.keras): \")\n    \n    if not filename:\n        print(\"Guardado cancelado.\")\n        return\n        \n    save_path = os.path.join(\"/kaggle/working/\", filename)\n    \n    try:\n        model.save(save_path)\n        print(f\"Modelo guardado exitosamente en {save_path}\")\n    except Exception as e:\n        print(f\"Error al guardar el modelo: {e}\")\n\ndef load_model_interactive():\n    print(\"Buscando modelos en '/kaggle/working/' y '/kaggle/input/...\")\n    filename = input(\"Ruta completa del archivo a cargar (ej: /kaggle/working/mi_modelo.keras): \")\n\n    if not filename:\n        print(\"Carga cancelada.\")\n        return None\n\n    if not os.path.exists(filename):\n        print(f\"Error: El archivo '{filename}' no existe.\")\n        return None\n        \n    try:\n        model = models.load_model(filename)\n        print(f\"Modelo cargado exitosamente desde {filename}\")\n        model.summary()\n        return model\n    except Exception as e:\n        print(f\"Error al cargar el modelo: {e}\")\n        return None\n\nprint(\"Funciones interactivas definidas.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T18:08:29.181154Z","iopub.execute_input":"2025-11-02T18:08:29.181435Z","iopub.status.idle":"2025-11-02T18:08:29.203307Z","shell.execute_reply.started":"2025-11-02T18:08:29.181414Z","shell.execute_reply":"2025-11-02T18:08:29.202260Z"}},"outputs":[{"name":"stdout","text":"Funciones interactivas definidas.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"model = None\nhistory = None\n\nif 'train_ds' not in locals() or 'val_ds' not in locals() or 'CLASS_NAMES' not in locals():\n    print(\"Error: Los datasets (train_ds, val_ds) no están cargados.\")\n    print(\"Por favor, asegúrate de ejecutar las Celdas 2 y 3 primero.\")\nelse:\n    print(\"Datasets (train_ds, val_ds) y función 'build_model' listos.\")\n    \n    while True:\n        print(\"\\n-------------------------------------------\")\n        print(\"  Panel de Control - Clasificador de Biomas\")\n        print(\"-------------------------------------------\")\n        \n        if model is None:\n            print(\"Estado: No hay modelo cargado en memoria.\")\n        else:\n            print(f\"Estado: Modelo '{model.name}' cargado.\")\n            \n        print(\"\\n--- Opciones Principales ---\")\n        print(\"1. Crear un nuevo modelo (borra el actual)\")\n        print(\"2. Cargar un modelo desde archivo\")\n        print(\"3. Entrenar el modelo actual\")\n        print(\"4. Evaluar (Probar) el modelo actual\")\n        print(\"5. Guardar el modelo actual\")\n        print(\"6. Salir\")\n        \n        main_choice = input(\"Seleccione una opción: \")\n        \n        if main_choice == '1':\n            print(\"\\nCreando nueva arquitectura de modelo...\")\n            model = build_model(NUM_CLASSES)\n            history = None\n            print(\"¡Nuevo modelo creado y listo para entrenar!\")\n            model.summary()\n\n        elif main_choice == '2':\n            loaded_model = load_model_interactive()\n            if loaded_model is not None:\n                model = loaded_model\n                history = None\n                print(\"Modelo reemplazado por el archivo cargado.\")\n\n        elif main_choice == '3':\n            if model is None:\n                print(\"Error: No hay modelo en memoria. Cree (Opción 1) o cargue (Opción 2) un modelo.\")\n            else:\n                model, history = train_model_interactive(model, train_ds, val_ds, history)\n\n        elif main_choice == '4':\n            if model is None:\n                print(\"Error: No hay modelo en memoria. Cree (Opción 1) o cargue (Opción 2) un modelo.\")\n            else:\n                test_model_interactive(model, val_ds, CLASS_NAMES)\n                \n        elif main_choice == '5':\n            if model is None:\n                print(\"Error: No hay modelo en memoria para guardar.\")\n            else:\n                save_model_interactive(model)\n                \n        elif main_choice == '6':\n            print(\"Saliendo del panel de control...\")\n            break\n            \n        else:\n            print(\"Opción no válida. Por favor, intente de nuevo.\")\n\nprint(\"Bucle del menú finalizado.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T18:08:29.204231Z","iopub.execute_input":"2025-11-02T18:08:29.204510Z","execution_failed":"2025-11-02T19:28:37.691Z"}},"outputs":[{"name":"stdout","text":"Datasets (train_ds, val_ds) y función 'build_model' listos.\n\n-------------------------------------------\n  Panel de Control - Clasificador de Biomas\n-------------------------------------------\nEstado: No hay modelo cargado en memoria.\n\n--- Opciones Principales ---\n1. Crear un nuevo modelo (borra el actual)\n2. Cargar un modelo desde archivo\n3. Entrenar el modelo actual\n4. Evaluar (Probar) el modelo actual\n5. Guardar el modelo actual\n6. Salir\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Seleccione una opción:  1\n"},{"name":"stdout","text":"\nCreando nueva arquitectura de modelo...\n¡Nuevo modelo creado y listo para entrenar!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"Minecraft_Biome_Classifier\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Minecraft_Biome_Classifier\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ rescaling_1 (\u001b[38;5;33mRescaling\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m18,496\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56320\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m28,836,352\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)             │         \u001b[38;5;34m4,369\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ rescaling_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56320</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">28,836,352</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,369</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,360,465\u001b[0m (112.00 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,360,465</span> (112.00 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,360,465\u001b[0m (112.00 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,360,465</span> (112.00 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n-------------------------------------------\n  Panel de Control - Clasificador de Biomas\n-------------------------------------------\nEstado: Modelo 'Minecraft_Biome_Classifier' cargado.\n\n--- Opciones Principales ---\n1. Crear un nuevo modelo (borra el actual)\n2. Cargar un modelo desde archivo\n3. Entrenar el modelo actual\n4. Evaluar (Probar) el modelo actual\n5. Guardar el modelo actual\n6. Salir\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Seleccione una opción:  3\nIngrese el número de épocas (ej. 10):  10\nIngrese la tasa de aprendizaje (ej. 0.001):  0.001\n"},{"name":"stdout","text":"\nEntrenando por 10 épocas con LR=0.001...\nEpoch 1/10\n\u001b[1m375/552\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m9:06\u001b[0m 3s/step - accuracy: 0.3737 - loss: 1.9975","output_type":"stream"}],"execution_count":null}]}