{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5692975,"sourceType":"datasetVersion","datasetId":3273348}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Agrupacion y verificacion del dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nfrom tqdm import tqdm\n\nBASE_INPUT_DIR = None\noriginal_biome_names = []\n\ntry:\n    dataset_root_walk = next(os.walk('/kaggle/input')) \n    if not dataset_root_walk[1] and dataset_root_walk[0] == '/kaggle/input':\n         dataset_root_walk = next(os.walk(dataset_root_walk[0]))\n\n    dataset_root_dir = dataset_root_walk[0]\n    preprocessed_data_path = os.path.join(dataset_root_dir, 'preprocessed_data')\n    \n    if os.path.isdir(preprocessed_data_path):\n        BASE_INPUT_DIR = preprocessed_data_path\n        original_biome_names = next(os.walk(BASE_INPUT_DIR))[1]\n        print(f\"Directorio base de entrada detectado: {BASE_INPUT_DIR}\")\n        print(f\"Se encontraron {len(original_biome_names)} directorios 'biome_X'.\")\n    else:\n        print(\"Advertencia: No se encontró 'preprocessed_data'. Buscando biomas en el directorio raíz del dataset.\")\n        BASE_INPUT_DIR = dataset_root_dir\n        original_biome_names = dataset_root_walk[1]\n        if not any(name.startswith('biome_') for name in original_biome_names):\n            raise FileNotFoundError(\"No se encontró 'preprocessed_data' ni directorios 'biome_X' en la raíz.\")\n            \nexcept (StopIteration, FileNotFoundError) as e:\n    print(f\"Error: No se encontró una estructura de directorios válida. {e}\")\n    print(\"Asegúrate de que el dataset (con 'preprocessed_data') esté agregado a este notebook.\")\n\nBASE_OUTPUT_DIR = '/kaggle/working/dataset_agrupado'\nos.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\nprint(f\"El dataset agrupado se guardará en: {BASE_OUTPUT_DIR}\")\n\n\nBIOME_MAP_BY_ID = {\n    \"Taiga\": [\"biome_5\", \"biome_19\", \"biome_32\", \"biome_33\", \"biome_133\"],\n    \"Taiga Nevada\": [\"biome_30\", \"biome_31\", \"biome_158\"],\n    \"Savana\": [\"biome_35\", \"biome_36\"],\n    \"Jungla\": [\"biome_21\", \"biome_22\"],\n    \"Bosque de Roble Oscuro\": [\"biome_29\", \"biome_157\"],\n    \"Desierto\": [\"biome_2\", \"biome_17\", \"biome_130\"],\n    \"Badlands\": [\"biome_37\", \"biome_38\", \"biome_39\"],\n    \"Bosque de Abeto\": [\"biome_27\", \"biome_28\", \"biome_156\"],\n    \"Pantano\": [\"biome_6\"],\n    \"Bosque de Roble\": [\"biome_4\", \"biome_132\"],\n    \"Planicies\": [\"biome_1\", \"biome_129\"],\n    \"Bosque Mixto\": [\"biome_18\", \"biome_34\"],\n    \"Tundra Nevada\": [\"biome_12\"],\n    \"Montañas\": [\"biome_3\", \"biome_131\", \"biome_162\"],\n    \"Montaña Nevada\": [\"biome_13\"],\n    \"Playa\": [\"biome_16\", \"biome_26\"],\n    \"Ríos\": [\"biome_7\", \"biome_11\"]\n}\n\nBIOMAS_EXCLUIDOS_BY_ID = [\"biome_10\", \"biome_45\"]\n\ntotal_images_processed = 0\nnew_category_counts = {category: 0 for category in BIOME_MAP_BY_ID.keys()}\noriginal_biomes_mapped = set()\n\nfor new_category, original_biome_id_list in tqdm(BIOME_MAP_BY_ID.items(), desc=\"Agrupando categorías\"): \n    new_category_dir = os.path.join(BASE_OUTPUT_DIR, new_category)\n    os.makedirs(new_category_dir, exist_ok=True)\n    \n    for old_biome_dir_name in original_biome_id_list:\n        original_biome_path = os.path.join(BASE_INPUT_DIR, old_biome_dir_name)\n        original_biomes_mapped.add(old_biome_dir_name)\n        \n        if not os.path.isdir(original_biome_path):\n            print(f\"  -> Advertencia: No se encontró el directorio para '{old_biome_dir_name}'. Omitiendo.\")\n            continue\n            \n        try:\n            images = os.listdir(original_biome_path)\n            for image_name in images:\n                source_path = os.path.join(original_biome_path, image_name)\n                dest_name = f\"{old_biome_dir_name}_{image_name}\"\n                dest_path = os.path.join(new_category_dir, dest_name)\n                \n                try:\n                    os.symlink(source_path, dest_path)\n                    total_images_processed += 1\n                    new_category_counts[new_category] += 1\n                except FileExistsError:\n                    pass \n                except Exception as e:\n                    print(f\"Error al crear enlace para {source_path}: {e}\")\n\n        except Exception as e:\n            print(f\"  -> Error procesando '{original_biome_path}': {e}\")\n\nprint(\"\\n--- Proceso de agrupación completado ---\")\nprint(f\"Total de imágenes enlazadas: {total_images_processed}\")\n\ntarget_counts = {\n    \"Taiga\": 2346, \"Taiga Nevada\": 420, \"Savana\": 1164, \"Jungla\": 444,\n    \"Bosque de Roble Oscuro\": 870, \"Desierto\": 2088, \"Badlands\": 210,\n    \"Bosque de Abeto\": 1122, \"Pantano\": 665, \"Bosque de Roble\": 2850,\n    \"Planicies\": 3186, \"Bosque Mixto\": 1320, \"Tundra Nevada\": 1254,\n    \"Montañas\": 2652, \"Montaña Nevada\": 474, \"Playa\": 540, \"Ríos\": 444\n}\nprint(\"\\n--- Verificación de Conteo de Imágenes ---\")\nfor category, count in new_category_counts.items():\n    target = target_counts.get(category, 0)\n    discrepancy = \"\"\n    if count != target:\n        discrepancy = f\" (Esperado: {target} - ¡Revisar!)\"\n    print(f\"- {category}: {count} imágenes {discrepancy}\")\n\nall_original_biomes_in_map = original_biomes_mapped | set(BIOMAS_EXCLUIDOS_BY_ID)\nunmapped_biomes = [b for b in original_biome_names if b not in all_original_biomes_in_map]\n        \nif unmapped_biomes:\n    print(f\"\\n¡ADVERTENCIA! Los siguientes directorios no fueron mapeados ni excluidos:\")\n    for b in unmapped_biomes: print(f\"  - {b}\")\nelse:\n    print(\"\\nTodos los directorios de biomas originales fueron mapeados o excluidos correctamente.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Division de los datos para entrenamiento y prueba","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, optimizers\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn.metrics as metrics\nimport seaborn as sns\nimport os\nimport sys\n\nIMG_HEIGHT = 180\nIMG_WIDTH = 320\nBATCH_SIZE = 32\nVALIDATION_SPLIT = 1/3 \nSEED = 123\nDATA_DIR = '/kaggle/working/dataset_agrupado'\n\nif not os.path.exists(DATA_DIR) or not os.listdir(DATA_DIR):\n    print(f\"Error: El directorio '{DATA_DIR}' está vacío o no existe.\")\n    print(\"Por favor, ejecuta la Celda 1 (agrupación de dataset) primero.\")\nelse:\n    CLASS_NAMES = sorted(os.listdir(DATA_DIR))\n    NUM_CLASSES = len(CLASS_NAMES)\n    print(f\"Configuración lista.\")\n    print(f\"División de datos: {1-VALIDATION_SPLIT:.2f} Entreno / {VALIDATION_SPLIT:.2f} Prueba\")\n    print(f\"Clases detectadas ({NUM_CLASSES}): {CLASS_NAMES}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Normalisacion del dataset haciendo los cambios","metadata":{}},{"cell_type":"code","source":"target_counts = {\n    \"Taiga\": 2346, \"Taiga Nevada\": 420, \"Savana\": 1164, \"Jungla\": 444,\n    \"Bosque de Roble Oscuro\": 870, \"Desierto\": 2088, \"Badlands\": 210,\n    \"Bosque de Abeto\": 1122, \"Pantano\": 665, \"Bosque de Roble\": 2850,\n    \"Planicies\": 3186, \"Bosque Mixto\": 1320, \"Tundra Nevada\": 1254,\n    \"Montañas\": 2652, \"Montaña Nevada\": 474, \"Playa\": 540, \"Ríos\": 444\n}\n\nMINORITY_THRESHOLD = 1000\nprint(f\"Umbral para aumento de datos: {MINORITY_THRESHOLD} imágenes.\")\n\nprint(\"Cargando dataset de entrenamiento (2/3)...\")\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    DATA_DIR,\n    validation_split=VALIDATION_SPLIT,\n    subset=\"training\",\n    seed=SEED,\n    image_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE\n)\n\nprint(\"Cargando dataset de prueba (1/3)...\")\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    DATA_DIR,\n    validation_split=VALIDATION_SPLIT,\n    subset=\"validation\",\n    seed=SEED,\n    image_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE\n)\n\nclass_names_loaded = train_ds.class_names\nminority_class_indices = []\n\nprint(\"Identificando clases minoritarias para aumento:\")\nfor i, class_name in enumerate(class_names_loaded):\n    count = target_counts.get(class_name, 0)\n    if count < MINORITY_THRESHOLD and count > 0:\n        minority_class_indices.append(i)\n        print(f\"  -> SÍ: {class_name} (índice {i}) con {count} imágenes.\")\n    else:\n        print(f\"  -> NO: {class_name} (índice {i}) con {count} imágenes.\")\n\nMINORITY_INDICES_TENSOR = tf.constant(minority_class_indices, dtype=tf.int32)\n\ndata_augmentation = keras.Sequential(\n    [\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.1),\n        layers.RandomZoom(0.1),\n        layers.RandomContrast(0.1),\n    ],\n    name=\"data_augmentation\",\n)\n\n@tf.function\ndef augment_if_minority(x, y):\n    x_augmented = data_augmentation(x, training=True)\n    y_cast = tf.cast(y, tf.int32)\n    y_reshaped = tf.reshape(y_cast, [-1, 1])\n    minority_reshaped = tf.reshape(MINORITY_INDICES_TENSOR, [1, -1])   \n    comparison_matrix = tf.equal(y_reshaped, minority_reshaped)\n    is_minority_vec = tf.reduce_any(comparison_matrix, axis=1)  \n    condition = tf.reshape(is_minority_vec, [-1, 1, 1, 1])\n    x_final = tf.where(condition, x_augmented, x)\n    return x_final, y\n\nprint(\"\\nAplicando aumento condicional SOLO a las clases minoritarias...\")\n\ntrain_ds = train_ds.map(\n    augment_if_minority,\n    num_parallel_calls=tf.data.AUTOTUNE\n)\n\ntrain_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n\nprint(\"\\nDatasets de entrenamiento (condicional) y prueba listos.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Constructor del modelo CNN","metadata":{}},{"cell_type":"code","source":"def build_model(num_classes, architecture_type=\"estandar\"):\n    model = models.Sequential()\n    model.add(layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n    model.add(layers.Rescaling(1./255)) # Normalización\n\n    if architecture_type == \"estandar\":\n        model.name = \"BiomeClassifier_Estandar\"\n        model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n        model.add(layers.MaxPooling2D((2, 2)))\n        model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n        model.add(layers.MaxPooling2D((2, 2)))\n        model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n        model.add(layers.MaxPooling2D((2, 2)))\n        model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n        model.add(layers.MaxPooling2D((2, 2)))\n        model.add(layers.Flatten())\n        model.add(layers.Dense(512, activation='relu'))\n        model.add(layers.Dropout(0.5))\n        model.add(layers.Dense(256, activation='relu'))\n        model.add(layers.Dropout(0.3))\n\n    elif architecture_type == \"simple\":\n        model.name = \"BiomeClassifier_Simple\"\n        model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n        model.add(layers.MaxPooling2D((2, 2)))\n        model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n        model.add(layers.MaxPooling2D((2, 2)))   \n        model.add(layers.Flatten())\n        model.add(layers.Dense(128, activation='relu'))\n        model.add(layers.Dropout(0.5))\n        \n    else:\n        raise ValueError(\"Tipo de arquitectura no reconocido. Use 'estandar' o 'simple'.\")\n\n    model.add(layers.Dense(num_classes, activation='softmax'))\n    \n    return model\n\nprint(\"Función 'build_model' (flexible) definida.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Entrenamiento del modelo ","metadata":{}},{"cell_type":"code","source":"def train_model_interactive(model, train_ds, val_ds, existing_history=None):\n    try:\n        epochs = int(input(\"Ingrese el número de épocas (ej. 10): \") or 10)\n        lr = float(input(\"Ingrese la tasa de aprendizaje (ej. 0.001): \") or 0.001)\n    except ValueError:\n        print(\"Error: Entrada no válida.\")\n        return model, existing_history\n\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=lr),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    print(f\"\\nEntrenando por {epochs} épocas con LR={lr}...\")\n    \n    history = model.fit(\n        train_ds,\n        epochs=epochs,\n        validation_data=val_ds,\n        verbose=1\n    )\n    \n    print(\"Entrenamiento finalizado.\")\n    \n    if existing_history:\n        for key in history.history:\n            if key in existing_history.history:\n                existing_history.history[key].extend(history.history[key])\n            else:\n                existing_history.history[key] = history.history[key]\n        plot_loss_history(existing_history)\n        return model, existing_history\n    else:\n        plot_loss_history(history)\n        return model, history\n\ndef plot_loss_history(history):\n    acc = history.history.get('accuracy', [])\n    val_acc = history.history.get('val_accuracy', [])\n    loss = history.history.get('loss', [])\n    val_loss = history.history.get('val_loss', [])\n    \n    if not acc or not val_acc or not loss or not val_loss:\n        print(\"No hay suficiente historial para graficar.\")\n        return\n\n    epochs_range = range(len(acc))\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, acc, label='Precisión de Entrenamiento')\n    plt.plot(epochs_range, val_acc, label='Precisión de Prueba')\n    plt.legend(loc='lower right')\n    plt.title('Precisión de Entrenamiento y Prueba')\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Pérdida de Entrenamiento')\n    plt.plot(epochs_range, val_loss, label='Pérdida de Prueba')\n    plt.legend(loc='upper right')\n    plt.title('Pérdida de Entrenamiento y Prueba')\n    plt.show()\n\ndef test_model_interactive(model, val_ds, class_names_list):\n    print(\"Evaluando modelo contra el conjunto de prueba (1/3)...\")\n    loss, accuracy = model.evaluate(val_ds, verbose=0)\n    print(\"\\n\" + \"=\"*60)\n    print(\"          RESULTADOS DE LA EVALUACIÓN\")\n    print(\"=\"*60)\n    print(f\"Pérdida (Loss): {loss:.4f}\")\n    print(f\"Precisión (Accuracy): {accuracy * 100:.2f}%\")\n    if accuracy > 0.90:\n        print(\"¡Éxito! Se alcanzó el objetivo de >90% de precisión.\")\n    else:\n        print(f\"Objetivo de >90% no alcanzado.\")\n    print(\"=\"*60)\n\n    print(\"\\nGenerando predicciones para el reporte detallado...\")\n    y_pred = []\n    y_true = []\n    for images, labels in val_ds:\n        y_true.extend(labels.numpy())\n        preds = model.predict(images, verbose=0)\n        y_pred.extend(np.argmax(preds, axis=1))\n      \n    print(\"\\n--- Reporte de Clasificación (Precisión, Recall, F1) ---\")\n    print(metrics.classification_report(y_true, y_pred, target_names=class_names_list, zero_division=0))\n    print(\"Generando Matriz de Confusión...\")\n    cm = metrics.confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(14, 12))\n    sns.heatmap(\n        cm, \n        annot=True, \n        fmt='d', \n        cmap='Blues', \n        xticklabels=class_names_list, \n        yticklabels=class_names_list\n    )\n    plt.title('Matriz de Confusión')\n    plt.ylabel('Bioma Real (True Label)')\n    plt.xlabel('Bioma Predicho (Predicted Label)')\n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0)\n    plt.show()\n\ndef save_model_interactive(model):\n    print(\"El modelo se guardará en '/kaggle/working/'\")\n    filename = input(\"Nombre del archivo (ej: mi_modelo.keras): \")\n    \n    if not filename:\n        print(\"Guardado cancelado.\")\n        return\n        \n    save_path = os.path.join(\"/kaggle/working/\", filename)\n    \n    try:\n        model.save(save_path)\n        print(f\"Modelo guardado exitosamente en {save_path}\")\n    except Exception as e:\n        print(f\"Error al guardar el modelo: {e}\")\n\ndef load_model_interactive():\n    print(\"Buscando modelos en '/kaggle/working/' y '/kaggle/input/...\")\n    filename = input(\"Ruta completa del archivo a cargar (ej: /kaggle/working/mi_modelo.keras): \")\n\n    if not filename:\n        print(\"Carga cancelada.\")\n        return None\n\n    if not os.path.exists(filename):\n        print(f\"Error: El archivo '{filename}' no existe.\")\n        return None\n        \n    try:\n        model = models.load_model(filename)\n        print(f\"Modelo cargado exitosamente desde {filename}\")\n        model.summary()\n        return model\n    except Exception as e:\n        print(f\"Error al cargar el modelo: {e}\")\n        return None\n\ndef predict_single_image_interactive(model, class_names_list):\n    print(\"\\n--- Predecir Imagen Individual ---\")\n    print(\"Instrucciones: Use la barra lateral de Kaggle (Data > Upload) para subir su imagen.\")\n    print(\"La imagen aparecerá en '/kaggle/working/'.\")\n    path = input(\"Ingrese la ruta de la imagen (ej: /kaggle/working/mi_foto.png): \")\n\n    if not os.path.exists(path):\n        print(f\"Error: No se encontró el archivo en '{path}'.\")\n        return\n\n    try:\n        img = tf.keras.utils.load_img(path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n        img_array = tf.keras.utils.img_to_array(img)\n        img_batch = tf.expand_dims(img_array, 0) # Crear un lote de 1\n        predictions = model.predict(img_batch, verbose=0)\n        score = tf.nn.softmax(predictions[0])  \n        predicted_index = np.argmax(score)\n        predicted_class = class_names_list[predicted_index]\n        confidence = 100 * np.max(score)\n\n        print(\"\\n--- Resultado ---\")\n        print(f\"La red cree que es: {predicted_class}\")\n        print(f\"Confianza: {confidence:.2f}%\")\n\n    except Exception as e:\n        print(f\"Error al procesar la imagen: {e}\")\n\nprint(\"Funciones interactivas (con predicción individual) definidas.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Menu principal","metadata":{}},{"cell_type":"code","source":"model = None\nhistory = None\n\nif 'train_ds' not in locals() or 'val_ds' not in locals() or 'CLASS_NAMES' not in locals():\n    print(\"Error: Los datasets (train_ds, val_ds) no están cargados.\")\n    print(\"Por favor, asegúrate de ejecutar las Celdas 2 y 3 primero.\")\nelse:\n    print(\"Datasets (train_ds, val_ds) y función 'build_model' listos.\")\n    arch_choice = input(\"Creación automática: ¿Qué arquitectura desea? (1: Estándar, 2: Simple) [Default: 1]: \") or \"1\"\n    arch_type = \"simple\" if arch_choice == \"2\" else \"estandar\"\n\n    print(f\"\\nCreando arquitectura de modelo inicial ({arch_type})...\")\n    model = build_model(NUM_CLASSES, architecture_type=arch_type)\n    history = None\n    print(\"¡Nuevo modelo creado y listo para entrenar!\")\n    model.summary()\n    \n    while True:\n        print(\"\\n-------------------------------------------\")\n        print(\"  Panel de Control - Clasificador de Biomas\")\n        print(\"-------------------------------------------\")\n        \n        if model is None:\n            print(\"Estado: No hay modelo cargado en memoria.\")\n        else:\n            print(f\"Estado: Modelo '{model.name}' cargado.\")\n        print(\"\\n--- Opciones Principales ---\")\n        print(\"1. Crear un nuevo modelo (borra el actual)\")\n        print(\"2. Cargar un modelo desde archivo\")\n        print(\"3. Entrenar el modelo actual\")\n        print(\"4. Evaluar (Probar) el modelo actual\")\n        print(\"5. Guardar el modelo actual\")\n        print(\"6. Predecir imagen individual\")\n        print(\"7. Salir\")\n        \n        main_choice = input(\"Seleccione una opción: \")\n        \n        if main_choice == '1':\n            arch_choice_menu = input(\"¿Qué arquitectura desea? (1: Estándar, 2: Simple) [Default: 1]: \") or \"1\"\n            arch_type_menu = \"simple\" if arch_choice_menu == \"2\" else \"estandar\"\n            print(f\"\\nCreando nueva arquitectura de modelo ({arch_type_menu})...\")\n            model = build_model(NUM_CLASSES, architecture_type=arch_type_menu)\n            history = None\n            print(\"¡Nuevo modelo creado y listo para entrenar!\")\n            model.summary()\n\n        elif main_choice == '2':\n            loaded_model = load_model_interactive()\n            if loaded_model is not None:\n                model = loaded_model\n                history = None\n                print(\"Modelo reemplazado por el archivo cargado.\")\n\n        elif main_choice == '3':\n            if model is None:\n                print(\"Error: No hay modelo en memoria. Cree (Opción 1) o cargue (Opción 2) un modelo.\")\n            else:\n                model, history = train_model_interactive(model, train_ds, val_ds, history)\n\n        elif main_choice == '4':\n            if model is None:\n                print(\"Error: No hay modelo en memoria. Cree (Opción 1) o cargue (Opción 2) un modelo.\")\n            else:\n                test_model_interactive(model, val_ds, CLASS_NAMES)            \n        elif main_choice == '5':\n            if model is None:\n                print(\"Error: No hay modelo en memoria para guardar.\")\n            else:\n                save_model_interactive(model)\n        elif main_choice == '6':\n            if model is None:\n                print(\"Error: No hay modelo en memoria. Cree (Opción 1) o cargue (Opción 2) un modelo.\")\n            else:\n                predict_single_image_interactive(model, CLASS_NAMES)\n        elif main_choice == '7':\n            print(\"Saliendo del panel de control...\")\n            break\n            \n        else:\n            print(\"Opción no válida. Por favor, intente de nuevo.\")\n\nprint(\"Bucle del menú finalizado.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}