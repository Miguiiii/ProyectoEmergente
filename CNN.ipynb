{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5692975,"sourceType":"datasetVersion","datasetId":3273348}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    classification_report,\n)\nimport os\nimport sys\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom PIL import Image\nimport datetime\n\nCARPETA_DATOS_AGRUPADOS = './dataset_agrupado'\nDIRECTORIO_DATOS = CARPETA_DATOS_AGRUPADOS\nDIRECTORIO_KAGGLE = \"/kaggle/input\"\n\nprint(f\"Buscando el directorio 'preprocessed_data' con subcarpetas 'biome_' dentro de {DIRECTORIO_KAGGLE}...\")\n\nDIRECTORIO_ENTRADA_BIOMAS = None\ntry:\n    for ruta_actual, carpetas, archivos in os.walk(DIRECTORIO_KAGGLE, topdown=True):\n        if os.path.basename(ruta_actual) == 'preprocessed_data':\n            if any(carpeta.startswith('biome_') for carpeta in carpetas):\n                DIRECTORIO_ENTRADA_BIOMAS = ruta_actual\n                print(f\"Directorio de biomas válido encontrado en: {DIRECTORIO_ENTRADA_BIOMAS}\")\n                print(f\"Subdirectorios de biomas detectados (muestra): {carpetas[:5]}\")\n                break\n    if DIRECTORIO_ENTRADA_BIOMAS is None:\n        print(\"--- ERROR FATAL (DETECCIÓN) ---\")\n        print(\"No se pudo encontrar un directorio 'preprocessed_data' válido que contenga subcarpetas 'biome_X'.\")\n        DIRECTORIO_ENTRADA_BIOMAS = \"/kaggle/input/DIRECTORIO_NO_ENCONTRADO\"\nexcept Exception as e:\n    print(f\"Error fatal al buscar directorios: {e}\")\n    DIRECTORIO_ENTRADA_BIOMAS = \"/kaggle/input/DIRECTORIO_NO_ENCONTRADO\"\n\nALTO_IMAGEN = 180\nANCHO_IMAGEN = 320\nTAMANO_LOTE = 32\nPROPORCION_VALIDACION = 1 / 3\nSEMILLA_ALEATORIA = 123\nUMBRAL_MINORIA = 1000\n\nMAPA_AGRUPACION_BIOMAS = {\n    \"Taiga\": [\"biome_5\", \"biome_19\", \"biome_32\", \"biome_33\", \"biome_133\"],\n    \"Taiga Nevada\": [\"biome_30\", \"biome_31\", \"biome_158\"],\n    \"Savana\": [\"biome_35\", \"biome_36\"],\n    \"Jungla\": [\"biome_21\", \"biome_22\"],\n    \"Bosque de Roble Oscuro\": [\"biome_29\", \"biome_157\"],\n    \"Desierto\": [\"biome_2\", \"biome_17\", \"biome_130\"],\n    \"Badlands\": [\"biome_37\", \"biome_38\", \"biome_39\"],\n    \"Bosque de Abeto\": [\"biome_27\", \"biome_28\", \"biome_156\"],\n    \"Pantano\": [\"biome_6\"],\n    \"Bosque de Roble\": [\"biome_4\", \"biome_132\"],\n    \"Planicies\": [\"biome_1\", \"biome_129\"],\n    \"Bosque Mixto\": [\"biome_18\", \"biome_34\"],\n    \"Tundra Nevada\": [\"biome_12\"],\n    \"Montañas\": [\"biome_3\", \"biome_131\", \"biome_162\"],\n    \"Montaña Nevada\": [\"biome_13\"],\n    \"Playa\": [\"biome_16\", \"biome_26\"],\n    \"Ríos\": [\"biome_7\", \"biome_11\"]\n}\n\nBIOMAS_IGNORADOS = [\"biome_10\", \"biome_45\"]\n\ndef preparar_y_agrupar_biomas():\n    print(f\"Directorio base de entrada detectado: {DIRECTORIO_ENTRADA_BIOMAS}\")\n    if not DIRECTORIO_ENTRADA_BIOMAS or not os.path.exists(DIRECTORIO_ENTRADA_BIOMAS) or not os.listdir(DIRECTORIO_ENTRADA_BIOMAS):\n        print(f\"--- ERROR FATAL ---\\nEl directorio '{DIRECTORIO_ENTRADA_BIOMAS}' no existe o está vacío.\")\n        return False\n\n    os.makedirs(CARPETA_DATOS_AGRUPADOS, exist_ok=True)\n    print(f\"El dataset agrupado se guardará en: {CARPETA_DATOS_AGRUPADOS}\")\n\n    try:\n        nombres_biomas_originales = next(os.walk(DIRECTORIO_ENTRADA_BIOMAS))[1]\n    except (StopIteration, FileNotFoundError) as e:\n        print(f\"Error: No se encontró una estructura de directorios válida en {DIRECTORIO_ENTRADA_BIOMAS}. {e}\")\n        return False\n\n    mapa_inverso_biomas = {}\n    for nueva_categoria, lista_biomas_antiguos in MAPA_AGRUPACION_BIOMAS.items():\n        for bioma_antiguo_id in lista_biomas_antiguos:\n            mapa_inverso_biomas[bioma_antiguo_id] = nueva_categoria\n\n    print(\"Creando directorios de salida para las categorías agrupadas...\")\n    todas_las_categorias = list(MAPA_AGRUPACION_BIOMAS.keys())\n    for nueva_categoria in todas_las_categorias:\n        os.makedirs(os.path.join(CARPETA_DATOS_AGRUPADOS, nueva_categoria), exist_ok=True)\n\n    biomas_originales_mapeados = set()\n    nombre_carpeta_raiz = os.path.basename(DIRECTORIO_ENTRADA_BIOMAS)\n\n    print(f\"Recorriendo {DIRECTORIO_ENTRADA_BIOMAS} en busca de imágenes...\")\n    for ruta_actual, _, archivos in tqdm(os.walk(DIRECTORIO_ENTRADA_BIOMAS), desc=\"Procesando biomas\"):\n        bioma_actual_id = os.path.basename(ruta_actual)\n\n        if bioma_actual_id == nombre_carpeta_raiz or bioma_actual_id in BIOMAS_IGNORADOS:\n            continue\n\n        if bioma_actual_id in mapa_inverso_biomas:\n            biomas_originales_mapeados.add(bioma_actual_id)\n            nueva_categoria = mapa_inverso_biomas[bioma_actual_id]\n            directorio_nueva_categoria = os.path.join(CARPETA_DATOS_AGRUPADOS, nueva_categoria)\n\n            for nombre_imagen in archivos:\n                if not nombre_imagen.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n                    continue\n\n                ruta_origen = os.path.join(ruta_actual, nombre_imagen)\n                nombre_destino = f\"{bioma_actual_id}_{nombre_imagen}\"\n                ruta_destino = os.path.join(directorio_nueva_categoria, nombre_destino)\n\n                if os.path.exists(ruta_destino):\n                    continue\n\n                try:\n                    os.symlink(ruta_origen, ruta_destino)\n                except (OSError, NotImplementedError, AttributeError):\n                    import shutil\n                    shutil.copy2(ruta_origen, ruta_destino)\n\n    print(\"\\n--- Proceso de agrupación completado ---\")\n    print(\"\\n--- Verificación de Conteo (Desde carpetas de destino) ---\")\n    conteo_final_categorias = {}\n    total_imagenes_agrupadas = 0\n\n    for categoria in todas_las_categorias:\n        ruta_categoria = os.path.join(CARPETA_DATOS_AGRUPADOS, categoria)\n        try:\n            archivos_en_categoria = os.listdir(ruta_categoria)\n            conteo = len([f for f in archivos_en_categoria if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n        except FileNotFoundError:\n            conteo = 0\n        conteo_final_categorias[categoria] = conteo\n    \n    total_imagenes_agrupadas = sum(conteo_final_categorias.values())\n    print(f\"Total de imágenes encontradas en las carpetas de destino: {total_imagenes_agrupadas}\")\n\n    if total_imagenes_agrupadas == 0:\n        print(\"0 imagenes procesadas. ¡Algo salió mal!\")\n    else:\n        print(\"\\nCantidad total de imagenes encontradas:\")\n        for categoria, conteo in conteo_final_categorias.items():\n            print(f\"- {categoria}: {conteo} imágenes\")\n\n    todos_biomas_originales_en_mapa = biomas_originales_mapeados | set(BIOMAS_IGNORADOS)\n    biomas_sin_mapa = [b for b in nombres_biomas_originales if b not in todos_biomas_originales_en_mapa]\n\n    if biomas_sin_mapa:\n        print(f\"\\n¡Atención! Los siguientes directorios no fueron mapeados ni excluidos:\")\n        for b in biomas_sin_mapa: print(f\"  - {b}\")\n    else:\n        print(\"\\n¡Perfecto! Todos los directorios fueron trabajados correctamente.\")\n    \n    return True\n\ndef cargar_y_preparar_datos(directorio_datos):\n    if not os.path.exists(directorio_datos) or not os.listdir(directorio_datos):\n        print(f\"Error: El directorio '{directorio_datos}' está vacío o no existe.\")\n        return None, None, None, None, 0\n\n    transformaciones_entrenamiento = transforms.Compose([\n        transforms.Resize((ALTO_IMAGEN, ANCHO_IMAGEN)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(10),\n        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n        transforms.RandomResizedCrop((ALTO_IMAGEN, ANCHO_IMAGEN), scale=(0.9, 1.0)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    transformaciones_validacion = transforms.Compose([\n        transforms.Resize((ALTO_IMAGEN, ANCHO_IMAGEN)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    print(\"  -> [FASE 2.1] Escaneando directorio de datos (ImageFolder)...\")\n    try:\n        dataset_completo = datasets.ImageFolder(directorio_datos)\n    except Exception as e:\n        print(f\"Error al cargar datos con ImageFolder desde '{directorio_datos}': {e}\")\n        return None, None, None, None, 0\n\n    print(f\"  -> [FASE 2.2] Escaneo completo. {len(dataset_completo)} imágenes encontradas.\")\n    nombres_clases = dataset_completo.classes\n    numero_de_clases = len(nombres_clases)\n    print(f\"Clases detectadas ({numero_de_clases}): {nombres_clases}\")\n\n    indices = list(range(len(dataset_completo)))\n    corte = int(np.floor(PROPORCION_VALIDACION * len(dataset_completo)))\n\n    np.random.seed(SEMILLA_ALEATORIA)\n    np.random.shuffle(indices)\n\n    indices_entrenamiento, indices_validacion = indices[corte:], indices[:corte]\n\n    class DatasetPersonalizado(Dataset):\n        def __init__(self, subset, transformacion):\n            self.subset = subset\n            self.transformacion = transformacion\n        def __getitem__(self, index):\n            imagen, etiqueta = self.subset[index]\n            imagen = self.transformacion(imagen)\n            return imagen, etiqueta\n        def __len__(self):\n            return len(self.subset)\n\n    datos_entrenamiento = DatasetPersonalizado(\n        torch.utils.data.Subset(dataset_completo, indices_entrenamiento),\n        transformacion=transformaciones_entrenamiento\n    )\n    datos_validacion = DatasetPersonalizado(\n        torch.utils.data.Subset(dataset_completo, indices_validacion),\n        transformacion=transformaciones_validacion\n    )\n\n    print(\"  -> [FASE 2.3] Calculando pesos para el muestreador (sampler)...\")\n\n    etiquetas_entrenamiento = []\n    for i in tqdm(indices_entrenamiento, desc=\"   - Recopilando etiquetas (targets)\"):\n        etiquetas_entrenamiento.append(dataset_completo.targets[i])\n\n    conteo_por_clase = np.bincount(etiquetas_entrenamiento, minlength=numero_de_clases)\n\n    for i, nombre_clase in enumerate(nombres_clases):\n        conteo = conteo_por_clase[i]\n        if conteo < UMBRAL_MINORIA and conteo > 0:\n            print(f\"  -> (Sobremuestreo): {nombre_clase} (índice {i}) con {conteo} imágenes.\")\n        else:\n            print(f\"  -> (Normal): {nombre_clase} (índice {i}) con {conteo} imágenes.\")\n\n    conteo_por_clase[conteo_por_clase == 0] = 1\n\n    pesos_por_clase = 1. / conteo_por_clase\n\n    lista_pesos_muestras = []\n    for t in tqdm(etiquetas_entrenamiento, desc=\"   - Calculando pesos (weights)\"):\n        lista_pesos_muestras.append(pesos_por_clase[t])\n\n    pesos_muestras_tensor = torch.from_numpy(np.array(lista_pesos_muestras)).double()\n\n    print(\"  -> [FASE 2.4] Creando DataLoader de Validación...\")\n    cargador_validacion = DataLoader(datos_validacion,\n                                   batch_size=TAMANO_LOTE,\n                                   shuffle=False,\n                                   num_workers=2,\n                                   pin_memory=True)\n\n    print(f\"\\nDatasets listos. Entreno: {len(datos_entrenamiento)}, Prueba: {len(datos_validacion)}\")\n    return datos_entrenamiento, pesos_muestras_tensor, cargador_validacion, nombres_clases, numero_de_clases\n\nclass ClasificadorBiomasCNN(nn.Module):\n    def __init__(self, numero_de_clases, tipo_arquitectura=\"estandar\"):\n        super(ClasificadorBiomasCNN, self).__init__()\n        self.tipo_arquitectura = tipo_arquitectura\n\n        capas_convolucionales = []\n        if tipo_arquitectura == \"estandar\":\n            capas_convolucionales.append(nn.Conv2d(3, 32, kernel_size=3, padding=1))\n            capas_convolucionales.append(nn.ReLU())\n            capas_convolucionales.append(nn.MaxPool2d(kernel_size=2, stride=2))\n            capas_convolucionales.append(nn.Conv2d(32, 64, kernel_size=3, padding=1))\n            capas_convolucionales.append(nn.ReLU())\n            capas_convolucionales.append(nn.MaxPool2d(kernel_size=2, stride=2))\n            capas_convolucionales.append(nn.Conv2d(64, 128, kernel_size=3, padding=1))\n            capas_convolucionales.append(nn.ReLU())\n            capas_convolucionales.append(nn.MaxPool2d(kernel_size=2, stride=2))\n            capas_convolucionales.append(nn.Conv2d(128, 256, kernel_size=3, padding=1))\n            capas_convolucionales.append(nn.ReLU())\n            capas_convolucionales.append(nn.MaxPool2d(kernel_size=2, stride=2))\n            self.tamano_vector_aplanado = 256 * 11 * 20\n\n        elif tipo_arquitectura == \"simple\":\n            capas_convolucionales.append(nn.Conv2d(3, 32, kernel_size=3, padding=1))\n            capas_convolucionales.append(nn.ReLU())\n            capas_convolucionales.append(nn.MaxPool2d(kernel_size=2, stride=2))\n            capas_convolucionales.append(nn.Conv2d(32, 64, kernel_size=3, padding=1))\n            capas_convolucionales.append(nn.ReLU())\n            capas_convolucionales.append(nn.MaxPool2d(kernel_size=2, stride=2))\n            self.tamano_vector_aplanado = 64 * 45 * 80\n\n        else:\n            raise ValueError(\"Tipo de arquitectura no reconocido. Use 'estandar' o 'simple'.\")\n\n        self.extractor_caracteristicas = nn.Sequential(*capas_convolucionales)\n\n        capas_clasificacion = []\n        if tipo_arquitectura == \"estandar\":\n            capas_clasificacion.append(nn.Linear(self.tamano_vector_aplanado, 512))\n            capas_clasificacion.append(nn.ReLU())\n            capas_clasificacion.append(nn.Dropout(0.5))\n            capas_clasificacion.append(nn.Linear(512, 256))\n            capas_clasificacion.append(nn.ReLU())\n            capas_clasificacion.append(nn.Dropout(0.3))\n            capas_clasificacion.append(nn.Linear(256, numero_de_clases))\n\n        elif tipo_arquitectura == \"simple\":\n            capas_clasificacion.append(nn.Linear(self.tamano_vector_aplanado, 128))\n            capas_clasificacion.append(nn.ReLU())\n            capas_clasificacion.append(nn.Dropout(0.5))\n            capas_clasificacion.append(nn.Linear(128, numero_de_clases))\n\n        self.clasificador_final = nn.Sequential(*capas_clasificacion)\n\n    def forward(self, imagen):\n        caracteristicas = self.extractor_caracteristicas(imagen)\n        vector_plano = torch.flatten(caracteristicas, 1)\n        puntuaciones = self.clasificador_final(vector_plano)\n        return puntuaciones\n\ndef entrenar_modelo(red_neuronal, cargador_entrenamiento, numero_epocas, tasa_aprendizaje, dispositivo):\n    funcion_de_perdida = nn.CrossEntropyLoss()\n    optimizador = optim.Adam(red_neuronal.parameters(), lr=tasa_aprendizaje)\n\n    historial_perdida = []\n    red_neuronal.to(dispositivo)\n\n    for epoca in range(numero_epocas):\n        perdida_acumulada = 0.0\n        \n        red_neuronal.train()\n\n        barra_progreso = tqdm(cargador_entrenamiento, desc=f\"Época {epoca+1}/{numero_epocas}\", unit=\"batch\")\n\n        for imagenes, etiquetas in barra_progreso:\n            imagenes, etiquetas = imagenes.to(dispositivo), etiquetas.to(dispositivo)\n\n            optimizador.zero_grad()\n            salidas = red_neuronal(imagenes)\n            perdida = funcion_de_perdida(salidas, etiquetas)\n            perdida.backward()\n            optimizador.step()\n\n            perdida_acumulada += perdida.item()\n            barra_progreso.set_postfix(loss=perdida.item())\n\n        perdida_promedio_epoca = perdida_acumulada / len(cargador_entrenamiento)\n        historial_perdida.append(perdida_promedio_epoca)\n        print(f\"Época {epoca+1}/{numero_epocas} completada. Loss Promedio: {perdida_promedio_epoca:.4f}\")\n\n    print(\"Entrenamiento finalizado.\")\n    return historial_perdida\n\ndef graficar_historial_perdida(historial_perdida):\n    plt.figure()\n    plt.plot(historial_perdida, label='Pérdida (Loss) de Entrenamiento')\n    plt.title(\"Historial de Pérdida (Loss) por Época\")\n    plt.xlabel(\"Época\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef evaluar_modelo(red_neuronal, cargador_pruebas, dispositivo, nombres_clases):\n    red_neuronal.to(dispositivo)\n    red_neuronal.eval()\n\n    todas_predicciones = []\n    todas_etiquetas_reales = []\n\n    print(\"Evaluando modelo contra el conjunto de prueba...\")\n    barra_progreso = tqdm(cargador_pruebas, desc=\"Evaluando\", unit=\"batch\")\n\n    with torch.no_grad():\n        for imagenes, etiquetas in barra_progreso:\n            imagenes, etiquetas = imagenes.to(dispositivo), etiquetas.to(dispositivo)\n\n            salidas = red_neuronal(imagenes)\n            _, prediccion = torch.max(salidas.data, 1)\n            \n            todas_predicciones.extend(prediccion.cpu().numpy())\n            todas_etiquetas_reales.extend(etiquetas.cpu().numpy())\n\n    accuracy = accuracy_score(todas_etiquetas_reales, todas_predicciones)\n    precision = precision_score(todas_etiquetas_reales, todas_predicciones, average='macro', zero_division=0)\n    recall = recall_score(todas_etiquetas_reales, todas_predicciones, average='macro', zero_division=0)\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"          RESULTADOS DE LA EVALUACIÓN (BIOMAS)\")\n    print(\"=\"*60)\n    print(f\"Accuracy (Exactitud): {accuracy:.4f} ({accuracy*100:.2f}%)\")\n    print(f\"Precision (Precisión - Macro): {precision:.4f} ({precision*100:.2f}%)\")\n    print(f\"Recall (Sensibilidad - Macro): {recall:.4f} ({recall*100:.2f}%)\")\n    print(\"=\"*60)\n\n    print(\"\\n--- Reporte de Clasificación (Precisión, Recall, F1) ---\")\n    print(classification_report(todas_etiquetas_reales, todas_predicciones, target_names=nombres_clases, zero_division=0))\n\n    print(\"Generando Matriz de Confusión...\")\n    matriz_confusion = confusion_matrix(todas_etiquetas_reales, todas_predicciones)\n    \n    plt.figure(figsize=(14, 12))\n    ax = sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues',\n                xticklabels=nombres_clases, yticklabels=nombres_clases, annot_kws={\"size\": 10})\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n    ax.set_xlabel(\"Bioma Predicho\", fontsize=12)\n    ax.set_ylabel(\"Bioma Real\", fontsize=12)\n    ax.set_title(\"Matriz de Confusión - Biomas\", fontsize=14, weight=\"bold\")\n    plt.tight_layout()\n    plt.show()\n    print(\"Gráfico de Matriz de Confusión generado.\")\n\ndef guardar_red_neuronal(red_neuronal, parametros_arquitectura, nombre_archivo):\n    estado_modelo = {\n        'state_dict': red_neuronal.to('cpu').state_dict(),\n        'arch_params': parametros_arquitectura\n    }\n    \n    ruta_guardado = os.path.basename(nombre_archivo)\n    if not ruta_guardado:\n        ruta_guardado = \"mi_modelo.pth\"\n        \n    torch.save(estado_modelo, ruta_guardado)\n    print(f\"Modelo guardado en {os.path.abspath(ruta_guardado)}\")\n\ndef cargar_red_neuronal(nombre_archivo, dispositivo):\n    if not os.path.exists(nombre_archivo):\n        print(f\"Error: El archivo '{nombre_archivo}' no existe.\")\n        return None, None\n        \n    try:\n        estado_cargado = torch.load(nombre_archivo, map_location=dispositivo)\n    except Exception as e:\n        print(f\"Error al leer el archivo. ¿Es un archivo .pth válido? ({e})\")\n        return None, None\n\n    if 'arch_params' not in estado_cargado or 'state_dict' not in estado_cargado:\n        print(\"Error: El archivo .pth no contiene 'arch_params' o 'state_dict'.\")\n        return None, None\n\n    parametros_arquitectura = estado_cargado['arch_params']\n    \n    red_neuronal = ClasificadorBiomasCNN(**parametros_arquitectura)\n    \n    red_neuronal.load_state_dict(estado_cargado['state_dict'])\n    \n    red_neuronal.to(dispositivo)\n    \n    print(f\"Modelo cargado desde {nombre_archivo} al dispositivo {dispositivo}\")\n    print(\"\\n--- Configuración de la Red Cargada ---\")\n    print(f\"Tipo: {parametros_arquitectura.get('tipo_arquitectura', 'N/A')}\")\n    print(f\"Salida (Clases): {parametros_arquitectura.get('numero_de_clases', 'N/A')}\")\n    print(red_neuronal)\n    print(\"--------------------------------------\")\n    \n    return red_neuronal, parametros_arquitectura\n\ndef buscar_imagenes_para_predecir(directorio_busqueda, directorio_a_excluir):\n    rutas_imagenes_encontradas = []\n    print(f\"Buscando imágenes de prueba en: {directorio_busqueda}\")\n    \n    excluir_dataset_dir = \"\"\n    if directorio_a_excluir and os.path.exists(directorio_a_excluir):\n        excluir_dataset_dir = os.path.abspath(directorio_a_excluir)\n        print(f\"Excluyendo directorio de dataset: {excluir_dataset_dir}\")\n        \n    excluir_output_dir = os.path.abspath(CARPETA_DATOS_AGRUPADOS)\n    print(f\"Excluyendo directorio de salida: {excluir_output_dir}\")\n\n    for ruta_actual, _, archivos in os.walk(directorio_busqueda, topdown=True):\n        ruta_absoluta = os.path.abspath(ruta_actual)\n        \n        if excluir_dataset_dir and ruta_absoluta.startswith(excluir_dataset_dir):\n            continue\n        if ruta_absoluta.startswith(excluir_output_dir):\n            continue\n            \n        for nombre_archivo in archivos:\n            if nombre_archivo.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n                ruta_completa = os.path.join(ruta_actual, nombre_archivo)\n                rutas_imagenes_encontradas.append(ruta_completa)\n                \n    return rutas_imagenes_encontradas\n\ndef predecir_imagen_individual(red_neuronal, nombres_clases, dispositivo):\n    print(\"\\n--- Predecir Imagen Individual (Desde Lista) ---\")\n    \n    imagenes_encontradas = buscar_imagenes_para_predecir(DIRECTORIO_KAGGLE, DIRECTORIO_ENTRADA_BIOMAS)\n    \n    if not imagenes_encontradas:\n        print(f\"No se encontraron imágenes de prueba (jpg, png) en '{DIRECTORIO_KAGGLE}'\")\n        print(f\"(Asegúrese de que NO estén dentro de '{DIRECTORIO_ENTRADA_BIOMAS}')\")\n        print(\"Por favor, añada un dataset que contenga imágenes de prueba (ej. 'test-images').\")\n        return\n\n    print(\"Imágenes de prueba encontradas:\")\n    for i, ruta_img in enumerate(imagenes_encontradas):\n        ruta_mostrada = ruta_img.replace(DIRECTORIO_KAGGLE, '.../input')\n        print(f\"  {i + 1}: {ruta_mostrada}\")\n\n    ruta_elegida = None\n    while ruta_elegida is None:\n        try:\n            opcion_str = input(f\"Seleccione el número de la imagen (1-{len(imagenes_encontradas)}): \")\n            opcion_int = int(opcion_str) - 1\n            if 0 <= opcion_int < len(imagenes_encontradas):\n                ruta_elegida = imagenes_encontradas[opcion_int]\n            else:\n                print(\"Número fuera de rango. Intente de nuevo.\")\n        except ValueError:\n            print(\"Entrada no válida. Por favor, ingrese un número.\")\n\n    print(f\"Cargando imagen seleccionada: {ruta_elegida}\")\n    try:\n        transformaciones_prediccion = transforms.Compose([\n            transforms.Resize((ALTO_IMAGEN, ANCHO_IMAGEN)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        imagen = Image.open(ruta_elegida).convert(\"RGB\")\n        imagen_tensor = transformaciones_prediccion(imagen)\n        imagen_lote = imagen_tensor.unsqueeze(0)\n\n        red_neuronal.to(dispositivo)\n        red_neuronal.eval()\n\n        with torch.no_grad():\n            salidas = red_neuronal(imagen_lote.to(dispositivo))\n            probabilidades = torch.nn.functional.softmax(salidas, dim=1)\n            puntuaciones = probabilidades[0]\n            confianza, indice_predicho = torch.max(puntuaciones, 0)\n\n        clase_predicha = nombres_clases[indice_predicho.item()]\n\n        print(\"\\n--- Resultado ---\")\n        print(f\"Archivo: {os.path.basename(ruta_elegida)}\")\n        print(f\"La red cree que es: {clase_predicha}\")\n        print(f\"Confianza: {confianza.item() * 100:.2f}%\")\n\n    except Exception as e:\n        print(f\"Error al procesar la imagen: {e}\")\n\ndef menu_interactivo_modelo(red_neuronal, parametros_arquitectura, datos_entrenamiento, pesos_muestras_tensor, cargador_validacion, nombres_clases, dispositivo):\n    historial_perdida = []\n    \n    while True:\n        print(\"\\n--- Menú de Red - Clasificador de Biomas ---\")\n        print(f\"Modelo en memoria: {parametros_arquitectura.get('tipo_arquitectura', 'N/A')}\")\n        print(\"1. Entrenar la red\")\n        print(\"2. Probar (Evaluar) la red\")\n        print(\"3. Predecir imagen individual\")\n        print(\"4. Guardar la red\")\n        print(\"5. Volver al menú principal\")\n        opcion = input(\"Seleccione una opción: \")\n\n        if opcion == '1':\n            try:\n                epocas = int(input(\"Ingrese el número de épocas (ej. 10): \") or 10)\n                lr = float(input(\"Ingrese la tasa de aprendizaje (ej. 0.001): \") or 0.001)\n                porcentaje_str = input(\"Porcentaje del dataset de entrenamiento a usar (1-100) [Default: 100]: \") or \"100\"\n                porcentaje = float(porcentaje_str) / 100.0\n                \n                if not (0.01 <= porcentaje <= 1.0):\n                    print(\"Porcentaje inválido. Usando 100%.\")\n                    porcentaje = 1.0\n                    \n                total_muestras_entrenamiento = len(pesos_muestras_tensor)\n                \n                if porcentaje < 1.0:\n                    muestras_a_usar = int(total_muestras_entrenamiento * porcentaje)\n                    print(f\"Usando {porcentaje*100:.0f}% del dataset. {muestras_a_usar} muestras por época.\")\n                else:\n                    muestras_a_usar = total_muestras_entrenamiento\n                    print(f\"Usando 100% del dataset. {muestras_a_usar} muestras por época.\")\n\n                muestreador_entrenamiento = torch.utils.data.WeightedRandomSampler(\n                    weights=pesos_muestras_tensor,\n                    num_samples=muestras_a_usar,\n                    replacement=True\n                )\n                \n                cargador_entrenamiento_actual = DataLoader(datos_entrenamiento,\n                                                          batch_size=TAMANO_LOTE,\n                                                          sampler=muestreador_entrenamiento,\n                                                          num_workers=2,\n                                                          pin_memory=True)\n\n                print(f\"Entrenando por {epocas} épocas con LR={lr} en {dispositivo}...\")\n                nueva_perdida = entrenar_modelo(red_neuronal, cargador_entrenamiento_actual, epocas, lr, dispositivo)\n                historial_perdida.extend(nueva_perdida)\n                graficar_historial_perdida(historial_perdida)\n\n            except ValueError:\n                print(\"Error: Entrada no válida.\")\n        \n        elif opcion == '2':\n            print(f\"Probando modelo contra el conjunto de prueba en {dispositivo}...\")\n            evaluar_modelo(red_neuronal, cargador_validacion, dispositivo, nombres_clases)\n        \n        elif opcion == '3':\n            predecir_imagen_individual(red_neuronal, nombres_clases, dispositivo)\n        \n        elif opcion == '4':\n            nombre_archivo = input(\"Ingrese el nombre para guardar el modelo (ej: mi_modelo.pth): \")\n            if not nombre_archivo.endswith(\".pth\"):\n                nombre_archivo += \".pth\"\n            if nombre_archivo:\n                guardar_red_neuronal(red_neuronal, parametros_arquitectura, nombre_archivo)\n            else:\n                print(\"Guardado cancelado (nombre vacío).\")\n        \n        elif opcion == '5':\n            print(\"Volviendo al menú principal...\")\n            break\n        \n        else:\n            print(\"Opción no válida. Intente de nuevo.\")\n\ndef menu_principal():\n    red_neuronal_activa = None\n    params_arquitectura_activos = None\n    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    while True:\n        print(\"\\n\" + \"=\"*50)\n        print(\"  Panel de Control - Clasificador de Biomas (PyTorch)\")\n        print(f\" (Usando dispositivo: {dispositivo})\")\n        print(\"=\"*50)\n        print(\"1. Preparar datos y Crear nueva red (CNN)\")\n        print(\"2. Cargar una red desde archivo (.pth)\")\n        print(\"3. Salir\")\n        opcion_principal = input(\"Seleccione una opción: \")\n\n        if opcion_principal == '1':\n            try:\n                print(\"\\n--- FASE 1: Agrupando y verificando el dataset ---\")\n                if not preparar_y_agrupar_biomas():\n                    print(\"Error crítico en la agrupación del dataset. Abortando.\")\n                    continue\n\n                print(\"\\n--- FASE 2: Cargando y preparando datasets (PyTorch) ---\")\n                datos_entrenamiento, pesos_muestras_tensor, cargador_validacion, nombres_clases, numero_de_clases = cargar_y_preparar_datos(DIRECTORIO_DATOS)\n                \n                if datos_entrenamiento is None:\n                    print(\"No se pudieron cargar los datos. Volviendo al menú.\")\n                    continue\n\n                print(\"\\n--- FASE 3: Creación de Modelo (CNN) ---\")\n                opcion_arquitectura = input(\"¿Qué arquitectura desea? (1: Estándar, 2: Simple) [Default: 1]: \") or \"1\"\n                tipo_arquitectura = \"simple\" if opcion_arquitectura == \"2\" else \"estandar\"\n                \n                params_arquitectura_activos = {\n                    'num_classes': numero_de_clases,\n                    'architecture_type': tipo_arquitectura\n                }\n                \n                red_neuronal_activa = ClasificadorBiomasCNN(**params_arquitectura_activos).to(dispositivo)\n\n                print(\"\\n--- Nueva Red CNN Creada ---\")\n                print(red_neuronal_activa)\n                print(\"------------------------------\")\n                \n                menu_interactivo_modelo(red_neuronal_activa, params_arquitectura_activos, datos_entrenamiento, pesos_muestras_tensor, cargador_validacion, nombres_clases, dispositivo)\n\n            except ValueError:\n                print(\"Error: Entrada numérica no válida.\")\n            except Exception as e:\n                print(f\"Ocurrió un error creando la red: {e}\")\n                import traceback\n                traceback.print_exc()\n\n        elif opcion_principal == '2':\n            print(\"\\n--- Cargar Modelo ---\")\n            print(\"Buscando archivos .pth en el directorio actual y subdirectorios...\")\n            archivos_pth_encontrados = []\n            for ruta_actual, _, archivos in os.walk('.'):\n                for f in archivos:\n                    if f.endswith(\".pth\"):\n                        archivos_pth_encontrados.append(os.path.join(ruta_actual, f))\n            \n            if not archivos_pth_encontrados:\n                print(\"No se encontraron archivos .pth.\")\n                print(\"Asegúrate de haber guardado un modelo previamente.\")\n                continue\n\n            print(\"Archivos .pth encontrados:\")\n            for i, ruta_archivo in enumerate(archivos_pth_encontrados):\n                print(f\"  {i + 1}: {ruta_archivo}\")\n            \n            nombre_archivo_cargado = None\n            while nombre_archivo_cargado is None:\n                try:\n                    opcion_str = input(f\"Seleccione el número del modelo a cargar (1-{len(archivos_pth_encontrados)}): \")\n                    opcion_int = int(opcion_str) - 1\n                    if 0 <= opcion_int < len(archivos_pth_encontrados):\n                        nombre_archivo_cargado = archivos_pth_encontrados[opcion_int]\n                    else:\n                        print(\"Número fuera de rango.\")\n                except ValueError:\n                    print(\"Entrada no válida.\")\n            \n            print(f\"Cargando modelo: {nombre_archivo_cargado}\")\n            try:\n                red_neuronal_activa, params_arquitectura_activos = cargar_red_neuronal(nombre_archivo_cargado, dispositivo)\n                \n                if red_neuronal_activa and params_arquitectura_activos:\n                    print(\"\\n--- FASE 1: Verificando datos (necesarios para la red cargada) ---\")\n                    if not os.path.exists(DIRECTORIO_DATOS) or not os.listdir(DIRECTORIO_DATOS):\n                        print(\"El directorio de datos agrupados no existe. Ejecutando agrupación...\")\n                        if not preparar_y_agrupar_biomas():\n                            print(\"Error crítico en la agrupación del dataset. Abortando.\")\n                            continue\n                    else:\n                        print(\"Directorio de datos agrupados encontrado.\")\n\n                    print(\"\\n--- FASE 2: Cargando y preparando datasets (PyTorch) ---\")\n                    datos_entrenamiento, pesos_muestras_tensor, cargador_validacion, nombres_clases, numero_de_clases = cargar_y_preparar_datos(DIRECTORIO_DATOS)\n                    \n                    if datos_entrenamiento is None:\n                        print(\"No se pudieron cargar los datos para la red. Volviendo al menú.\")\n                        continue\n                    \n                    if numero_de_clases != params_arquitectura_activos['num_classes']:\n                        print(f\"¡ADVERTENCIA! El modelo fue entrenado con {params_arquitectura_activos['num_classes']} clases,\")\n                        print(f\"pero los datos en '{DIRECTORIO_DATOS}' tienen {numero_de_clases} clases.\")\n                        print(\"Esto causará un error. Asegúrate de que los datos agrupados coincidan con el modelo.\")\n                        continue\n                        \n                    menu_interactivo_modelo(red_neuronal_activa, params_arquitectura_activos, datos_entrenamiento, pesos_muestras_tensor, cargador_validacion, nombres_clases, dispositivo)\n\n            except FileNotFoundError:\n                print(f\"Error: El archivo '{nombre_archivo_cargado}' no fue encontrado.\")\n            except Exception as e:\n                print(f\"Ocurrió un error al cargar o probar la red: {e}\")\n                import traceback\n                traceback.print_exc()\n\n        elif opcion_principal == '3':\n            print(\"Saliendo del programa.\")\n            break\n        \n        else:\n            print(\"Opción no válida. Por favor, intente de nuevo.\")\n\nif __name__ == \"__main__\":\n    np.random.seed(SEMILLA_ALEATORIA)\n    torch.manual_seed(SEMILLA_ALEATORIA)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(SEMILLA_ALEATORIA)\n\n    tipo_ejecucion_kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Unknown')\n\n    if tipo_ejecucion_kaggle == 'Interactive':\n        print(\"--- MODO INTERACTIVO (DRAFT) ---\")\n        print(\"Iniciando menú principal. Podrá usar 'input()'.\\n\")\n        menu_principal()\n    \n    else:\n        if tipo_ejecucion_kaggle == 'Batch':\n            print(\"--- MODO NO INTERACTIVO (COMMIT / RUN ALL) ---\")\n        else:\n            print(f\"--- MODO NO INTERACTIVO (Fallback, KAGGLE_KERNEL_RUN_TYPE={tipo_ejecucion_kaggle}) ---\")\n        \n        print(\"Iniciando flujo automático (sin inputs).\\n\")\n        \n        modelo_automatico = None\n        params_arquitectura_auto = None\n        historial_perdida_auto = []\n        dispositivo_auto = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\" (Usando dispositivo: {dispositivo_auto})\")\n\n        try:\n            print(\"\\n--- FASE 1: Agrupando y verificando el dataset ---\")\n            if not preparar_y_agrupar_biomas():\n                print(\"Error crítico en la agrupación del dataset. Abortando.\")\n                sys.exit(1)\n\n            print(\"\\n--- FASE 2: Cargando y preparando datasets (PyTorch) ---\")\n            datos_entrenamiento, pesos_muestras_tensor, cargador_validacion, nombres_clases, numero_de_clases = cargar_y_preparar_datos(DIRECTORIO_DATOS)\n            if datos_entrenamiento is None:\n                print(\"No se pudieron cargar los datos. Abortando.\")\n                sys.exit(1)\n\n            print(\"\\n--- FASE 3: Creación de Modelo (CNN) ---\")\n            tipo_arquitectura_auto = \"estandar\"\n            print(f\"Arquitectura seleccionada: {tipo_arquitectura_auto}\")\n            params_arquitectura_auto = {\n                'num_classes': numero_de_clases,\n                'architecture_type': tipo_arquitectura_auto\n            }\n            modelo_automatico = ClasificadorBiomasCNN(**params_arquitectura_auto).to(dispositivo_auto)\n            print(\"\\n--- Nueva Red CNN Creada ---\")\n\n            epocas_auto = 100\n            lr_auto = 0.001\n            porcentaje_auto = 1.0\n            print(f\"\\n--- FASE 4: Iniciando Entrenamiento ---\")\n            print(f\"Épocas: {epocas_auto}, LR: {lr_auto}, Dataset: {porcentaje_auto*100:.0f}%\")\n            \n            total_muestras = len(pesos_muestras_tensor)\n            muestras_a_usar = int(total_muestras * porcentaje_auto)\n            muestreador_auto = torch.utils.data.WeightedRandomSampler(\n                weights=pesos_muestras_tensor,\n                num_samples=muestras_a_usar,\n                replacement=True\n            )\n            cargador_entrenamiento_auto = DataLoader(datos_entrenamiento,\n                                                    batch_size=TAMANO_LOTE,\n                                                    sampler=muestreador_auto,\n                                                    num_workers=2,\n                                                    pin_memory=True)\n            \n            nueva_perdida_auto = entrenar_modelo(modelo_automatico, cargador_entrenamiento_auto, epocas_auto, lr_auto, dispositivo_auto)\n            historial_perdida_auto.extend(nueva_perdida_auto)\n            \n            print(\"Generando gráfico de pérdida...\")\n            graficar_historial_perdida(historial_perdida_auto)\n\n            print(f\"\\n--- FASE 5: Probando modelo en {dispositivo_auto} ---\")\n            evaluar_modelo(modelo_automatico, cargador_validacion, dispositivo_auto, nombres_clases)\n\n            print(\"\\n--- FASE 6: Guardando modelo ---\")\n            marca_temporal = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            nombre_archivo_modelo = f\"auto_model_estandar_{marca_temporal}.pth\"\n            guardar_red_neuronal(modelo_automatico, params_arquitectura_auto, nombre_archivo_modelo)\n            \n            print(\"\\n--- FLUJO AUTOMÁTICO COMPLETADO ---\")\n\n        except Exception as e:\n            print(f\"Ocurrió un error en el flujo automático: {e}\")\n            import traceback\n            traceback.print_exc()\n            sys.exit(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}